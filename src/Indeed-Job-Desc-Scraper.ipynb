{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indeed Job Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "1. Get all of the URL's of EACH LISTING. for each page teh MAIN function gets, get the LINKS for that page (the job postings). For example 15 pages gets a list of 150 URLS. \n",
    "- for EACH page\n",
    "    - get Mosaic job card link\n",
    "    - iterate through the list of job cards\n",
    "    - append list of URLS by each individual URL\n",
    "    - move to next page in Indeed\n",
    "2. Look through the entire URL list and process them individually.\n",
    "- for each URL\n",
    "- extract the parts of the page for features \n",
    "3. On each individual page, 'write a row' for the data about the job as each features.\n",
    "\n",
    "Loop through list of URLs\n",
    "for each URLs, append a list by a dictionary of its contents\n",
    "dump into pd df\n",
    "pd.to_pickle('filepath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(position, location,page_count):\n",
    "        if page_count > 0:\n",
    "            template = 'https://www.indeed.com/jobs?q={}&l={}&start={}'\n",
    "            page_count *= 10\n",
    "            url = template.format(position, location,page_count)\n",
    "        else:\n",
    "            template = 'https://www.indeed.com/jobs?q={}&l={}'\n",
    "            url = template.format(position, location)\n",
    "        \n",
    "        return 'http://api.scraperapi.com?api_key=f39a27865b56e74c0423ab1b9ecf1f23&url=' + url\n",
    "\n",
    "def job_listings_generator(position, location,page_count=10):\n",
    "    headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cache-control': 'max-age=0',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'none',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.67 Safari/537.36 Edg/87.0.664.47'\n",
    "}\n",
    "    \n",
    "    urls = [] \n",
    "    i = 0\n",
    "    \n",
    "    # POPULATE LIST OF URLs\n",
    "    \n",
    "    while i < page_count:\n",
    "        \n",
    "        url = get_url(position, location,i)  # create the url while passing in the position and location.\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        cards = soup.find( id = \"mosaic-zone-jobcards\" )\n",
    "        \n",
    "        if cards is None:\n",
    "            print('EMPTY URL HERE! ')\n",
    "            print(url)\n",
    "            time.sleep(6.8)\n",
    "            continue\n",
    "            \n",
    "        cards = cards.find_all('a')\n",
    "        cards_test = [card for card in cards if 'pagead' in card['href'] or '/rc/' in card['href']]\n",
    "        \n",
    "        for card in cards:\n",
    "            record = card['href']\n",
    "\n",
    "\n",
    "            if \"pagead\" in record:\n",
    "                urls.append(\"https://www.indeed.com\"+record)\n",
    "\n",
    "            if \"/rc/\" in record:\n",
    "                urls.append(\"https://www.indeed.com\"+record)\n",
    "        \n",
    "        time.sleep(0.1)\n",
    "        i += 1\n",
    "    \n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_info(url,title,location):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        time.sleep(5.0)\n",
    "        try:\n",
    "            print('trying again!')\n",
    "            response = requests.get(url)\n",
    "        except Exception as e:\n",
    "            return {}\n",
    "        \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    d = dict()\n",
    "    \n",
    "    \n",
    "    #location\n",
    "    d[\"location\"]=location\n",
    "    \n",
    "    #title\n",
    "    d[\"job_title\"]=title\n",
    "\n",
    "    \n",
    "    job_info = soup.find(class_=\"jobsearch-ViewJobLayout-jobDisplay icl-Grid-col icl-u-xs-span12 icl-u-lg-span7\" )\n",
    "    \n",
    "#     print(job_info)\n",
    "    \n",
    "    \n",
    "    if job_info is None:\n",
    "        d[\"job_desc\"] = \"\"\n",
    "        time.sleep(6.8)\n",
    "        return d\n",
    "        \n",
    "    cards = job_info.find_all('h2')\n",
    "    \n",
    "    if len(cards) == 0:\n",
    "        d[\"job_desc\"] = \"\"\n",
    "        time.sleep(6.8)\n",
    "        return d\n",
    "            \n",
    "\n",
    "    #description\n",
    "    for card in cards:\n",
    "        d[\"job_desc\"]=card.find_next('div').text\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls = []\n",
    "# locations = [\"major metros / areas\"]\n",
    "# titles = [\"10+ job titles\"]\n",
    "\n",
    "titles = [\"data+scientist\",\"financial+analyst\", \"underwriter\",\"chemical+engineer\",\"physician\",\"university+recruiter\"]\n",
    "locations = [\"San+Francisco\",\"Portland\", \"Seattle\",\"New+York\",\"Los+Angeles\",\"Austin\"]\n",
    "\n",
    "urls = []\n",
    "job_information = []\n",
    "\n",
    "\n",
    "for title in titles:\n",
    "    for location in locations:\n",
    "        urls=job_listings_generator(title,location,8)\n",
    "        for job in urls:\n",
    "            job_information.append(job_info(job,title,location))\n",
    "            \n",
    "            time.sleep(0.1)\n",
    "            \n",
    "        print(f\"finished {location} listings for {title}!\")\n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(job_information)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a074e4f1e1e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/webscraped_pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "df.to_pickle(os.getcwd()+'/webscraped_pickle')\n",
    "df.to_csv('New_scrape')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
